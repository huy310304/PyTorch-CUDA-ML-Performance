# Optimizing Machine Learning with CUDA: A Comparative Study with PyTorch 

## üìù [A brief summarization poster](https://drive.google.com/file/d/1-79boy7_EZHLIIbIy23dW9GVcFYegx6w/view?usp=sharing)

## Research Focus

This project aims to compare the performance of a PyTorch machine learning model and a CUDA machine learning model generated using Large Language Models (LLMs). The primary goal is to evaluate the optimization capabilities of LLMs in generating CUDA code for machine learning tasks, identify any limitations, and determine areas for improvement. Additionally, this research seeks to provide guidance on framework selection for different ML tasks based on the findings.

## Recent Work and Updates (Fall 2024)
- Explored optimization techniques for minimizing synchronization overhead, applying these methods to compare performance between PyTorch and CUDA models.
- Integrated GPU-accelerated libraries, such as cuML from the RAPIDS suite, to extend the comparison with PyTorch, focusing on performance gains in various machine learning tasks.

## Tools and Methods

- **Programming Languages**: Python, NVIDIA CUDA
- **Machine Learning Framework**: PyTorch and CUDA ML models from scratch
- **Large Language Models (LLMs)**: ChatGPT4.0 (for generating CUDA code)
- **Profiling Tools**: 
  - *NVIDIA Nsight Systems*: For comprehensive profiling and system-level analysis of CUDA applications.
  - *NVIDIA Nsight Compute*: For detailed kernel profiling and optimization analysis in CUDA.
  - *Perfetto and PyTorch Profiler*: For profiling and analyzing PyTorch models, focusing on performance metrics such as execution time and memory usage.
- **Hardware**: 
  - *NVIDIA RTX 2080 Ti GPUs*: Utilized for running CUDA and PyTorch models.
  - *UVA GPU servers*: The primary computing resource for running and profiling experiments.
- **Visualization**:
  - *Matplotlib*: Used for creating performance comparison charts and visualizations to clearly display the results.


## Phase 1: Learning CUDA and Optimization Techniques

### Objective: 
Learn the fundamentals of CUDA programming, implement simple CUDA applications, and use profiling tools to analyze and optimize these applications.

### Activities:
- **Vector Addition Example**:
  - Manually implemented vector addition in both Python and CUDA.
  - Compared the performance of the Python vs. CUDA implementations to demonstrate the speedup achieved through GPU acceleration.
  - Applied simple optimization techniques (e.g., memory coalescing, kernel tuning) to further enhance the CUDA implementation.
  
- **Profiling and Visualization**:
  - Used NVIDIA Nsight tools to profile the CUDA implementation.
  - Created visualizations showing the impact of various optimizations on execution time and resource utilization.

### Outcome:
This phase successfully demonstrated the significant performance gains achievable through GPU acceleration using CUDA compared to Python implementations. It provided foundational knowledge in CUDA programming and optimization techniques, setting the stage for more advanced analysis in subsequent phases.

<br>

## Phase 2: Optimizing LLM-Generated CUDA Code

### Objective:
To enhance the performance of CUDA code generated by Large Language Models (LLMs) by applying optimization techniques and comparing it with manually optimized CUDA implementations.

### Activities:
- **Code Generation**:
  - Used ChatGPT to generate CUDA code for tasks like vector addition, matrix multiplication, and simple forward neural network.
  - Assessed initial performance focusing on execution time and memory usage.

- **Optimization**:
  - Identified inefficiencies in the LLM-generated code, such as memory access patterns and lack of kernel fusion.
  - Applied optimizations, including memory coalescing, kernel fusion, and shared memory usage, to improve performance.

- **Performance Profiling**:
  - Profiled the optimized code using Nsight Systems and Nsight Compute.
  - Documented improvements in execution time, memory usage, and GPU utilization.

- **Comparison**:
  - Compared the optimized LLM-generated CUDA code with manually written CUDA implementations.
  - Highlighted strengths, weaknesses, and areas for further improvement.
    
### Outcome:
This phase aimed to demonstrate the potential and limitations of LLMs in generating CUDA code that, with targeted optimizations, can achieve performance close to manually optimized implementations. The insights gained from this phase will inform the development of more sophisticated prompts and techniques for guiding LLMs in future code generation tasks.

<br>

## Phase 3: Performance Comparison for PyTorch and CUDA Simple ML Models

### Objective:
To assess the performance differences between PyTorch and CUDA implementations of simple machine learning models. 

### Activities:
- **Model Implementation**: Developed basic ML models (e.g., linear regression, single-layer neural networks) in both PyTorch and CUDA.
- **Performance Profiling**: Employed Nsight Systems and Nsight Compute to profile execution time, memory usage, and GPU utilization for both implementations.
- **Optimization Analysis**: Use CUDA-specific optimizations technques and codes in Phase 2 to compare the performance gains to those achieved with PyTorch.
- **Benchmarking**: Conducted benchmarking by varying input sizes, batch sizes, and data types to assess how each framework handles different scenarios.
- **Result Visualization**: Generated visual comparisons of training time, inference time, accuracy, and other key performance metrics to highlight the strengths and weaknesses of each approach.

### Outcome:
This phase provided a comprehensive comparison between PyTorch and CUDA implementations of simple machine learning models. It highlighted the areas where CUDA excels in performance, particularly with GPU-specific optimizations, and where PyTorch offers ease of use and flexibility. The findings from this phase contribute to making informed decisions on framework selection for various machine learning tasks.

## Future Work

- **Scalability**: Extend the comparison to more complex models and larger datasets.
- **Explore Benchmark**: Implement more advanced tasks like image classification and GANs to broaden the scope of comparison.
- **Enhance CUDA Techniques**: Apply more advanced CUDA optimization strategies for better performance.
- **Advanced LLM Utilization**: Investigate how to better guide LLMs in generating highly optimized CUDA code.
- **Extended Comparisons**: Consider comparing CUDA models with other machine learning frameworks like TensorFlow.

## Acknowledgments

- Special thanks to Professor Adwait Jog for guidance and support.
- Acknowledgment to the Department of Computer Science at the University of Virginia for providing the necessary resources and GPU servers.

# Initial CUDA ML Models from LLMs

The goal of this project is to implement a linear regression model to predict an output \( y \) given input \( X \) using the formula: \( y = w \times X + b \), where \( w \) is the weight and \( b \) is the bias.

This project focuses on the performance analysis of a simple regression task implemented in CUDA. The initial implementation was generated using ChatGPT and subsequently manually optimized to enhance execution time and accuracy. This README outlines the project structure, methodology, and results from the optimization process.

## Raw Model Features

- **CUDA Kernels**: Implements key operations including the forward pass, loss calculation, gradient computation, and weight updates for linear regression.
- **Training Loop**: Runs for 200 epochs, applying gradient descent to update model parameters and outputs training/testing loss every 10 epochs.
- **Parallelism**: Leverages CUDAâ€™s parallel execution to efficiently handle large datasets by splitting tasks across multiple threads.
- **Memory Management**: Allocates and manages device memory for inputs, predictions, parameters, and gradients. Uses CUDA events to measure and report training time.
- **Loss Function**: Utilizes Mean Absolute Error (MAE) for robust loss calculation.
- **Performance Profiling**: Includes CUDA events to measure execution time, providing insights into the model's training efficiency.

## Optimizations Applied

A series of optimizations were applied to the initial regression model generated by ChatGPT. Each optimization technique is listed below:

![Optimization Methods](optimization_methods.png)

The optimizations significantly improved the model's performance, reducing execution time and improving efficiency. The following graph illustrates the impact of these optimizations on execution time:

![Execution Time](execution_time.png)

### Showcase of Optimization Techniques

- **CUDA Streams**: Enabled concurrent memory transfers and kernel execution by overlapping data processing with non-blocking and segmented transfers. This approach enhances throughput and reduces idle times.
  
  ![Concurrent streams](concurrent_streams.png)

- **Fused Kernels**: Combined multiple operations into a single kernel launch, reducing overhead and leading to faster execution and improved GPU efficiency.

  ![Fused Kernels](fused_kernel.png)

### Insights and Future Work

While simple ML models like this linear regression can benefit from increased epochs to improve accuracy, more complex models require careful tuning and optimization, which may not be fully achievable through automatic generation by LLMs. Therefore, the focus in this phase has been on optimizing training time rather than solely improving accuracy.

Future work will involve scaling up the problem to more complex models and datasets, where both accuracy and efficiency will be evaluated and optimized.

This phase demonstrates the potential of CUDA for accelerating machine learning tasks, but also highlights the importance of manual optimization in achieving the best performance.
